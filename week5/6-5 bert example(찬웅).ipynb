{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지', '가', '방', '에', '들어가', '신다']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "tokenizer.morphs(\"아버지가방에들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('방', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('들어가', 'VV'),\n",
       " ('신다', 'EP+EC')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pos(\"아버지가방에들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "komoran 사용 예시\n",
      "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']\n",
      "[('아버지', 'NNG'), ('가방', 'NNP'), ('에', 'JKB'), ('들어가', 'VV'), ('시', 'EP'), ('ㄴ다', 'EC')]\n",
      "\n",
      "\n",
      "okt 사용 예시\n",
      "['아버지', '가방', '에', '들어가신다']\n",
      "[('아버지', 'Noun'), ('가방', 'Noun'), ('에', 'Josa'), ('들어가신다', 'Verb')]\n",
      "\n",
      "\n",
      "mecab 사용 예시\n",
      "['아버지', '가', '방', '에', '들어가', '신다']\n",
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EC')]\n",
      "\n",
      "\n",
      "hannanum 사용 예시\n",
      "['아버지가방에들어가', '이', '시ㄴ다']\n",
      "[('아버지가방에들어가', 'N'), ('이', 'J'), ('시ㄴ다', 'E')]\n",
      "\n",
      "\n",
      "kkma 사용 예시\n",
      "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']\n",
      "[('아버지', 'NNG'), ('가방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('시', 'EPH'), ('ㄴ다', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt,Komoran,Mecab,Hannanum,Kkma\n",
    "\n",
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == 'komoran':\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == 'okt':\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == 'mecab':\n",
    "        tokenizer = Mecab()\n",
    "    elif tokenizer_name == 'hannanum':\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == 'kkma':\n",
    "        tokenizer = Kkma()\n",
    "    else:\n",
    "        tokenizer = Mecab()\n",
    "    return tokenizer\n",
    "\n",
    "print(\"komoran 사용 예시\")\n",
    "tokenizer = get_tokenizer('komoran')\n",
    "print(tokenizer.morphs(\"아버지가방에들어가신다\"))\n",
    "print(tokenizer.pos(\"아버지가방에들어가신다\"))\n",
    "print(\"\\n\")\n",
    "print(\"okt 사용 예시\")\n",
    "tokenizer = get_tokenizer('okt')\n",
    "print(tokenizer.morphs(\"아버지가방에들어가신다\"))\n",
    "print(tokenizer.pos(\"아버지가방에들어가신다\"))\n",
    "print(\"\\n\")\n",
    "print(\"mecab 사용 예시\")\n",
    "tokenizer = get_tokenizer('mecab')\n",
    "print(tokenizer.morphs(\"아버지가방에들어가신다\"))\n",
    "print(tokenizer.pos(\"아버지가방에들어가신다\"))\n",
    "print(\"\\n\")\n",
    "print(\"hannanum 사용 예시\")\n",
    "tokenizer = get_tokenizer('hannanum')\n",
    "print(tokenizer.morphs(\"아버지가방에들어가신다\"))\n",
    "print(tokenizer.pos(\"아버지가방에들어가신다\"))\n",
    "print(\"\\n\")\n",
    "print(\"kkma 사용 예시\")\n",
    "tokenizer = get_tokenizer('kkma')\n",
    "print(tokenizer.morphs(\"아버지가방에들어가신다\"))\n",
    "print(tokenizer.pos(\"아버지가방에들어가신다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi\n",
    "tokenizer = KhaiiiApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer.analyze(\"아버지가방에들어가신다\")\n",
    "tokens =[]\n",
    "for word in data:\n",
    "    tokens.extend([str(m).split(\"/\")[0] for m in word.morphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지/NNG', '가/JKS', '방/NNG', '에/JKB', '들어가/VV', '시/EP', 'ㄴ다/EC']\n"
     ]
    }
   ],
   "source": [
    "data = tokenizer.analyze(\"아버지가방에들어가신다\")\n",
    "tokens =[]\n",
    "for word in data:\n",
    "    tokens.extend([str(m) for m in word.morphs])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.741 Gbse memory 0.558 Gb\n",
      "['애비는', '종이었다']\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "\n",
    "corpus_fname = \"processed_ratings.txt\"\n",
    "model_fname = \"soyword.model\"\n",
    "\n",
    "sentences = [sent.strip() for sent in open(corpus_fname, 'r').readlines()]\n",
    "word_extractor = WordExtractor(min_frequency=100, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)\n",
    "word_extractor.train(sentences)\n",
    "word_extractor.save(model_fname)\n",
    "token = tokenizer.tokenize(\"애비는 종이었다\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 6130\n",
      "all branching entropies was computed # words = 123575\n",
      "all accessor variety was computed # words = 123575\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "model_fname = \"soyword.model\"\n",
    "\n",
    "word_extractor = WordExtractor(min_frequency=100, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)\n",
    "word_extractor.load(model_fname)\n",
    "scores = word_extractor.word_scores()\n",
    "scores = {key:(scores[key].cohesion_forward*math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}\n",
    "tokenizer = LTokenizer(scores=scores)\n",
    "token = tokenizer.tokenize(\"애비는 종이었다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['애비는', '종이었다']\n"
     ]
    }
   ],
   "source": [
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
