{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류를 위한 파이프라인 만들기\n",
    "\n",
    "## 네이버 영화 리뷰 말뭉치의 이진분류 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bert tokenization import Full Tokenizer\n",
    "import collections\n",
    "import re\n",
    "import unicodedata\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    elif isinstance(text, unicode):\n",
    "      return text\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "    \n",
    "    \n",
    "def load_vocab(vocab_file):\n",
    "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "  vocab = collections.OrderedDict()\n",
    "  index = 0\n",
    "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "    while True:\n",
    "      token = convert_to_unicode(reader.readline())\n",
    "      if not token:\n",
    "        break\n",
    "      token = token.strip()\n",
    "      vocab[token] = index\n",
    "      index += 1\n",
    "  return vocab\n",
    "\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "  text = text.strip()\n",
    "  if not text:\n",
    "    return []\n",
    "  tokens = text.split()\n",
    "  return tokens\n",
    "\n",
    "\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "  def __init__(self, do_lower_case=True):\n",
    "    \"\"\"Constructs a BasicTokenizer.\n",
    "\n",
    "    Args:\n",
    "      do_lower_case: Whether to lower case the input.\n",
    "    \"\"\"\n",
    "    self.do_lower_case = do_lower_case\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "    text = convert_to_unicode(text)\n",
    "    text = self._clean_text(text)\n",
    "\n",
    "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "    # models. This is also applied to the English models now, but it doesn't\n",
    "    # matter since the English models were not trained on any Chinese data\n",
    "    # and generally don't have any Chinese data in them (there are Chinese\n",
    "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "    # words in the English Wikipedia.).\n",
    "    text = self._tokenize_chinese_chars(text)\n",
    "\n",
    "    orig_tokens = whitespace_tokenize(text)\n",
    "    split_tokens = []\n",
    "    for token in orig_tokens:\n",
    "      if self.do_lower_case:\n",
    "        token = token.lower()\n",
    "        token = self._run_strip_accents(token)\n",
    "      split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "    return output_tokens\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "    self.vocab = vocab\n",
    "    self.unk_token = unk_token\n",
    "    self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "\n",
    "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "    using the given vocabulary.\n",
    "\n",
    "    For example:\n",
    "      input = \"unaffable\"\n",
    "      output = [\"un\", \"##aff\", \"##able\"]\n",
    "\n",
    "    Args:\n",
    "      text: A single token or whitespace separated tokens. This should have\n",
    "        already been passed through `BasicTokenizer.\n",
    "\n",
    "    Returns:\n",
    "      A list of wordpiece tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    text = convert_to_unicode(text)\n",
    "\n",
    "    output_tokens = []\n",
    "    for token in whitespace_tokenize(text):\n",
    "      chars = list(token)\n",
    "      if len(chars) > self.max_input_chars_per_word:\n",
    "        output_tokens.append(self.unk_token)\n",
    "        continue\n",
    "\n",
    "      is_bad = False\n",
    "      start = 0\n",
    "      sub_tokens = []\n",
    "      while start < len(chars):\n",
    "        end = len(chars)\n",
    "        cur_substr = None\n",
    "        while start < end:\n",
    "          substr = \"\".join(chars[start:end])\n",
    "          if start > 0:\n",
    "            substr = \"##\" + substr\n",
    "          if substr in self.vocab:\n",
    "            cur_substr = substr\n",
    "            break\n",
    "          end -= 1\n",
    "        if cur_substr is None:\n",
    "          # 오타 음절(아래에서 '및')이고 해당 음절이 vocab에 존재하지 않으면\n",
    "          # 해당 토큰 전체를 UNK 토큰으로 치환하게 돼 성능 급격히 저하\n",
    "          # 예 : chars = \"너무재밓었다그래서보는것을추천한다\"인 경우 전체를 UNK 처리\n",
    "          # is_bad = True\n",
    "          # break\n",
    "          start += 1\n",
    "          continue\n",
    "        sub_tokens.append(cur_substr)\n",
    "        start = end\n",
    "\n",
    "      if is_bad:\n",
    "        output_tokens.append(self.unk_token)\n",
    "      else:\n",
    "        output_tokens.extend(sub_tokens)\n",
    "    return output_tokens\n",
    "\n",
    "\n",
    "class FullTokenizer(object):\n",
    "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_file, do_lower_case=True):\n",
    "    self.vocab = load_vocab(vocab_file)\n",
    "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    split_tokens = []\n",
    "    for token in self.basic_tokenizer.tokenize(text):\n",
    "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "        split_tokens.append(sub_token)\n",
    "\n",
    "    return split_tokens\n",
    "\n",
    "  def convert_tokens_to_ids(self, tokens):\n",
    "    return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "  def convert_ids_to_tokens(self, ids):\n",
    "    return convert_by_vocab(self.inv_vocab, ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'khaiii'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-79b3e8ad64e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkhaiii\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKhaiiiApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKomoran\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKkma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'khaiii'"
     ]
    }
   ],
   "source": [
    "import sys, re, argparse\n",
    "from khaiii import KhaiiiApi\n",
    "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
    "\n",
    "\n",
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == \"komoran\":\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == \"okt\":\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == \"mecab\":\n",
    "        tokenizer = Mecab()\n",
    "    elif tokenizer_name == \"hannanum\":\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == \"kkma\":\n",
    "        tokenizer = Kkma()\n",
    "    elif tokenizer_name == \"khaiii\":\n",
    "        tokenizer = KhaiiiApi()\n",
    "    else:\n",
    "        tokenizer = Mecab()\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def post_processing(tokens):\n",
    "    results = []\n",
    "    for token in tokens:\n",
    "        # 숫자에 공백을 주어서 띄우기\n",
    "        processed_token = [el for el in re.sub(r\"(\\d)\", r\" \\1 \", token).split(\" \") if len(el) > 0]\n",
    "        results.extend(processed_token)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner(object):\n",
    "\n",
    "    def __init__(self, train_corpus_fname=None, tokenized_train_corpus_fname=None,\n",
    "                 test_corpus_fname=None, tokenized_test_corpus_fname=None,\n",
    "                 model_name=\"bert\", model_save_path=None, vocab_fname=None, eval_every=1000,\n",
    "                 batch_size=32, num_epochs=10, dropout_keep_prob_rate=0.9, model_ckpt_path=None,\n",
    "                 sp_model_path=None):\n",
    "        # configurations\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "        self.model_name = model_name #임베딩은 무엇을 쓸지\n",
    "        self.eval_every = eval_every #평가는 얼마나 자주?\n",
    "        self.model_ckpt_path = model_ckpt_path\n",
    "        self.model_save_path = model_save_path\n",
    "        self.batch_size = batch_size #배치 데이터 크기\n",
    "        self.num_epochs = num_epochs #학습 에폭 수\n",
    "        self.dropout_keep_prob_rate = dropout_keep_prob_rate #드롭아웃 비율\n",
    "        self.best_valid_score = 0.0\n",
    "        if not os.path.exists(model_save_path):\n",
    "            os.mkdir(model_save_path)\n",
    "        # define tokenizer\n",
    "        if self.model_name == \"bert\":\n",
    "            self.tokenizer = FullTokenizer(vocab_file=vocab_fname, do_lower_case=False)\n",
    "        elif self.model_name == \"xlnet\":\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.Load(sp_model_path)\n",
    "            self.tokenizer = sp\n",
    "        else:\n",
    "            self.tokenizer = get_tokenizer(\"mecab\")\n",
    "        # load or tokenize corpus\n",
    "        self.train_data, self.train_data_size = self.load_or_tokenize_corpus(train_corpus_fname, tokenized_train_corpus_fname)\n",
    "        self.test_data, self.test_data_size = self.load_or_tokenize_corpus(test_corpus_fname, tokenized_test_corpus_fname)\n",
    "\n",
    "    def load_or_tokenize_corpus(self, corpus_fname, tokenized_corpus_fname):\n",
    "        data_set = []\n",
    "        if os.path.exists(tokenized_corpus_fname):\n",
    "            tf.logging.info(\"load tokenized corpus : \" + tokenized_corpus_fname)\n",
    "            with open(tokenized_corpus_fname, 'r') as f1:\n",
    "                for line in f1:\n",
    "                    tokens, label = line.strip().split(\"\\u241E\")\n",
    "                    if len(tokens) > 0:\n",
    "                        data_set.append([tokens.split(\" \"), int(label)])\n",
    "        else:\n",
    "            tf.logging.info(\"tokenize corpus : \" + corpus_fname + \" > \" + tokenized_corpus_fname)\n",
    "            with open(corpus_fname, 'r') as f2:\n",
    "                next(f2)  # skip head line\n",
    "                for line in f2:\n",
    "                    sentence, label = line.strip().split(\"\\u241E\")\n",
    "                    if self.model_name == \"bert\":\n",
    "                        tokens = self.tokenizer.tokenize(sentence)\n",
    "                    elif self.model_name == \"xlnet\":\n",
    "                        normalized_sentence = preprocess_text(sentence, lower=False)\n",
    "                        tokens = encode_pieces(self.tokenizer, normalized_sentence, return_unicode=False, sample=False)\n",
    "                    else:\n",
    "                        tokens = self.tokenizer.morphs(sentence)\n",
    "                        tokens = post_processing(tokens)\n",
    "                    if int(label) > 0.5:\n",
    "                        int_label = 1\n",
    "                    else:\n",
    "                        int_label = 0\n",
    "                    data_set.append([tokens, int_label])\n",
    "            with open(tokenized_corpus_fname, 'w') as f3:\n",
    "                for tokens, label in data_set:\n",
    "                    f3.writelines(' '.join(tokens) + \"\\u241E\" + str(label) + \"\\n\")\n",
    "        return data_set, len(data_set)\n",
    "\n",
    "    def train(self, sess, saver, global_step, output_feed):\n",
    "        train_batches = self.get_batch(self.train_data, num_epochs=self.num_epochs, is_training=True)\n",
    "        checkpoint_loss = 0.0\n",
    "        for current_input_feed in train_batches:\n",
    "            _, _, _, current_loss = sess.run(output_feed, current_input_feed)\n",
    "            checkpoint_loss += current_loss\n",
    "            if global_step.eval(sess) % self.eval_every == 0:\n",
    "                tf.logging.info(\"global step %d train loss %.4f\" %\n",
    "                                (global_step.eval(sess), checkpoint_loss / self.eval_every))\n",
    "                checkpoint_loss = 0.0\n",
    "                self.validation(sess, saver, global_step)\n",
    "\n",
    "    def validation(self, sess, saver, global_step):\n",
    "        valid_loss, valid_pred, valid_num_data = 0, 0, 0\n",
    "        output_feed = [self.logits, self.loss]\n",
    "        test_batches = self.get_batch(self.test_data, num_epochs=1, is_training=False)\n",
    "        for current_input_feed, current_labels in test_batches:\n",
    "            current_logits, current_loss = sess.run(output_feed, current_input_feed)\n",
    "            current_preds = np.argmax(current_logits, axis=-1)\n",
    "            valid_loss += current_loss\n",
    "            valid_num_data += len(current_labels)\n",
    "            for pred, label in zip(current_preds, current_labels):\n",
    "                if pred == label:\n",
    "                    valid_pred += 1\n",
    "        valid_score = valid_pred / valid_num_data\n",
    "        tf.logging.info(\"valid loss %.4f valid score %.4f\" %\n",
    "                        (valid_loss, valid_score))\n",
    "        if valid_score > self.best_valid_score:\n",
    "            self.best_valid_score = valid_score\n",
    "            path = self.model_save_path + \"/\" + str(valid_score)\n",
    "            saver.save(sess, path, global_step=global_step)\n",
    "\n",
    "    def get_batch(self, data, num_epochs, is_training=True):\n",
    "        if is_training:\n",
    "            data_size = self.train_data_size\n",
    "        else:\n",
    "            data_size = self.test_data_size\n",
    "        num_batches_per_epoch = int((data_size - 1) / self.batch_size)\n",
    "        if is_training:\n",
    "            tf.logging.info(\"num_batches_per_epoch : \" + str(num_batches_per_epoch))\n",
    "        for epoch in range(num_epochs):\n",
    "            idx = random.sample(range(data_size), data_size)\n",
    "            data = np.array(data)[idx]\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                batch_sentences = []\n",
    "                batch_labels = []\n",
    "                start_index = batch_num * self.batch_size\n",
    "                end_index = (batch_num + 1) * self.batch_size\n",
    "                features = data[start_index:end_index]\n",
    "                for feature in features:\n",
    "                    sentence, label = feature\n",
    "                    batch_sentences.append(sentence)\n",
    "                    batch_labels.append(int(label))\n",
    "                yield self.make_input(batch_sentences, batch_labels, is_training)\n",
    "\n",
    "    def make_input(self, sentences, labels, is_training):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def tune(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo 네트워크 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.bilm import Batcher, BidirectionalLanguageModel, weight_layers\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "DTYPE = 'float32'\n",
    "DTYPE_INT = 'int64'\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    '''\n",
    "    A token vocabulary.  Holds a map from token to ids and provides\n",
    "    a method for encoding text to a sequence of ids.\n",
    "    '''\n",
    "    def __init__(self, filename, validate_file=False):\n",
    "        '''\n",
    "        filename = the vocabulary file.  It is a flat text file with one\n",
    "            (normalized) token per line.  In addition, the file should also\n",
    "            contain the special tokens <S>, </S>, <UNK> (case sensitive).\n",
    "        '''\n",
    "        self._id_to_word = []\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._bos = -1\n",
    "        self._eos = -1\n",
    "\n",
    "        with open(filename) as f:\n",
    "            idx = 0\n",
    "            for line in f:\n",
    "                word_name = line.strip()\n",
    "                if word_name == '<S>':\n",
    "                    self._bos = idx\n",
    "                elif word_name == '</S>':\n",
    "                    self._eos = idx\n",
    "                elif word_name == '<UNK>':\n",
    "                    self._unk = idx\n",
    "                if word_name == '!!!MAXTERMID':\n",
    "                    continue\n",
    "\n",
    "                self._id_to_word.append(word_name)\n",
    "                self._word_to_id[word_name] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # check to ensure file has special tokens\n",
    "        if validate_file:\n",
    "            if self._bos == -1 or self._eos == -1 or self._unk == -1:\n",
    "                raise ValueError(\"Ensure the vocabulary file has \"\n",
    "                                 \"<S>, </S>, <UNK> tokens\")\n",
    "\n",
    "    @property\n",
    "    def bos(self):\n",
    "        return self._bos\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._id_to_word)\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        if word in self._word_to_id:\n",
    "            return self._word_to_id[word]\n",
    "        return self.unk\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word[cur_id]\n",
    "\n",
    "    def decode(self, cur_ids):\n",
    "        \"\"\"Convert a list of ids to a sentence, with space inserted.\"\"\"\n",
    "        return ' '.join([self.id_to_word(cur_id) for cur_id in cur_ids])\n",
    "\n",
    "    def encode(self, sentence, reverse=False, split=True):\n",
    "        \"\"\"Convert a sentence to a list of ids, with special tokens added.\n",
    "        Sentence is a single string with tokens separated by whitespace.\n",
    "\n",
    "        If reverse, then the sentence is assumed to be reversed, and\n",
    "            this method will swap the BOS/EOS tokens appropriately.\"\"\"\n",
    "\n",
    "        if split:\n",
    "            word_ids = [\n",
    "                self.word_to_id(cur_word) for cur_word in sentence.split()\n",
    "            ]\n",
    "        else:\n",
    "            word_ids = [self.word_to_id(cur_word) for cur_word in sentence]\n",
    "\n",
    "        if reverse:\n",
    "            return np.array([self.eos] + word_ids + [self.bos], dtype=np.int32)\n",
    "        else:\n",
    "            return np.array([self.bos] + word_ids + [self.eos], dtype=np.int32)\n",
    "\n",
    "        \n",
    "        \n",
    "class UnicodeCharsVocabulary(Vocabulary):\n",
    "    \"\"\"Vocabulary containing character-level and word level information.\n",
    "\n",
    "    Has a word vocabulary that is used to lookup word ids and\n",
    "    a character id that is used to map words to arrays of character ids.\n",
    "\n",
    "    The character ids are defined by ord(c) for c in word.encode('utf-8')\n",
    "    This limits the total number of possible char ids to 256.\n",
    "    To this we add 5 additional special ids: begin sentence, end sentence,\n",
    "        begin word, end word and padding.\n",
    "\n",
    "    WARNING: for prediction, we add +1 to the output ids from this\n",
    "    class to create a special padding id (=0).  As a result, we suggest\n",
    "    you use the `Batcher`, `TokenBatcher`, and `LMDataset` classes instead\n",
    "    of this lower level class.  If you are using this lower level class,\n",
    "    then be sure to add the +1 appropriately, otherwise embeddings computed\n",
    "    from the pre-trained model will be useless.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, max_word_length, **kwargs):\n",
    "        super(UnicodeCharsVocabulary, self).__init__(filename, **kwargs)\n",
    "        self._max_word_length = max_word_length\n",
    "\n",
    "        # char ids 0-255 come from utf-8 encoding bytes\n",
    "        # assign 256-300 to special chars\n",
    "        self.bos_char = 256  # <begin sentence>\n",
    "        self.eos_char = 257  # <end sentence>\n",
    "        self.bow_char = 258  # <begin word>\n",
    "        self.eow_char = 259  # <end word>\n",
    "        self.pad_char = 260 # <padding>\n",
    "\n",
    "        num_words = len(self._id_to_word)\n",
    "\n",
    "        self._word_char_ids = np.zeros([num_words, max_word_length],\n",
    "            dtype=np.int32)\n",
    "\n",
    "        # the charcter representation of the begin/end of sentence characters\n",
    "        def _make_bos_eos(c):\n",
    "            r = np.zeros([self.max_word_length], dtype=np.int32)\n",
    "            r[:] = self.pad_char\n",
    "            r[0] = self.bow_char\n",
    "            r[1] = c\n",
    "            r[2] = self.eow_char\n",
    "            return r\n",
    "        self.bos_chars = _make_bos_eos(self.bos_char)\n",
    "        self.eos_chars = _make_bos_eos(self.eos_char)\n",
    "\n",
    "        for i, word in enumerate(self._id_to_word):\n",
    "            self._word_char_ids[i] = self._convert_word_to_char_ids(word)\n",
    "\n",
    "        self._word_char_ids[self.bos] = self.bos_chars\n",
    "        self._word_char_ids[self.eos] = self.eos_chars\n",
    "        # TODO: properly handle <UNK>\n",
    "\n",
    "    @property\n",
    "    def word_char_ids(self):\n",
    "        return self._word_char_ids\n",
    "\n",
    "    @property\n",
    "    def max_word_length(self):\n",
    "        return self._max_word_length\n",
    "\n",
    "    def _convert_word_to_char_ids(self, word):\n",
    "        code = np.zeros([self.max_word_length], dtype=np.int32)\n",
    "        code[:] = self.pad_char\n",
    "\n",
    "        word_encoded = word.encode('utf-8', 'ignore')[:(self.max_word_length-2)]\n",
    "        code[0] = self.bow_char\n",
    "        for k, chr_id in enumerate(word_encoded, start=1):\n",
    "            code[k] = chr_id\n",
    "        code[len(word_encoded) + 1] = self.eow_char\n",
    "\n",
    "        return code\n",
    "\n",
    "    def word_to_char_ids(self, word):\n",
    "        if word in self._word_to_id:\n",
    "            return self._word_char_ids[self._word_to_id[word]]\n",
    "        else:\n",
    "            return self._convert_word_to_char_ids(word)\n",
    "\n",
    "    def encode_chars(self, sentence, reverse=False, split=True):\n",
    "        '''\n",
    "        Encode the sentence as a white space delimited string of tokens.\n",
    "        '''\n",
    "        if split:\n",
    "            chars_ids = [self.word_to_char_ids(cur_word)\n",
    "                     for cur_word in sentence.split()]\n",
    "        else:\n",
    "            chars_ids = [self.word_to_char_ids(cur_word)\n",
    "                     for cur_word in sentence]\n",
    "        if reverse:\n",
    "            return np.vstack([self.eos_chars] + chars_ids + [self.bos_chars])\n",
    "        else:\n",
    "            return np.vstack([self.bos_chars] + chars_ids + [self.eos_chars])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "class Batcher(object):\n",
    "    ''' \n",
    "    Batch sentences of tokenized text into character id matrices.\n",
    "    '''\n",
    "    def __init__(self, lm_vocab_file: str, max_token_length: int):\n",
    "        '''\n",
    "        lm_vocab_file = the language model vocabulary file (one line per\n",
    "            token)\n",
    "        max_token_length = the maximum number of characters in each token\n",
    "        '''\n",
    "        self._lm_vocab = UnicodeCharsVocabulary(\n",
    "            lm_vocab_file, max_token_length\n",
    "        )\n",
    "        self._max_token_length = max_token_length\n",
    "\n",
    "    def batch_sentences(self, sentences: List[List[str]]):\n",
    "        '''\n",
    "        Batch the sentences as character ids\n",
    "        Each sentence is a list of tokens without <s> or </s>, e.g.\n",
    "        [['The', 'first', 'sentence', '.'], ['Second', '.']]\n",
    "        '''\n",
    "        n_sentences = len(sentences)\n",
    "        max_length = max(len(sentence) for sentence in sentences) + 2\n",
    "\n",
    "        X_char_ids = np.zeros(\n",
    "            (n_sentences, max_length, self._max_token_length),\n",
    "            dtype=np.int64\n",
    "        )\n",
    "\n",
    "        for k, sent in enumerate(sentences):\n",
    "            length = len(sent) + 2\n",
    "            char_ids_without_mask = self._lm_vocab.encode_chars(\n",
    "                sent, split=False)\n",
    "            # add one so that 0 is the mask value\n",
    "            X_char_ids[k, :length, :] = char_ids_without_mask + 1\n",
    "\n",
    "        return X_char_ids\n",
    "    \n",
    "    \n",
    "    \n",
    "class BidirectionalLanguageModel(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            options_file: str,\n",
    "            weight_file: str,\n",
    "            use_character_inputs=True,\n",
    "            embedding_weight_file=None,\n",
    "            max_batch_size=128,\n",
    "        ):\n",
    "        '''\n",
    "        Creates the language model computational graph and loads weights\n",
    "\n",
    "        Two options for input type:\n",
    "            (1) To use character inputs (paired with Batcher)\n",
    "                pass use_character_inputs=True, and ids_placeholder\n",
    "                of shape (None, None, max_characters_per_token)\n",
    "                to __call__\n",
    "            (2) To use token ids as input (paired with TokenBatcher),\n",
    "                pass use_character_inputs=False and ids_placeholder\n",
    "                of shape (None, None) to __call__.\n",
    "                In this case, embedding_weight_file is also required input\n",
    "\n",
    "        options_file: location of the json formatted file with\n",
    "                      LM hyperparameters\n",
    "        weight_file: location of the hdf5 file with LM weights\n",
    "        use_character_inputs: if True, then use character ids as input,\n",
    "            otherwise use token ids\n",
    "        max_batch_size: the maximum allowable batch size \n",
    "        '''\n",
    "        with open(options_file, 'r') as fin:\n",
    "            options = json.load(fin)\n",
    "\n",
    "        if not use_character_inputs:\n",
    "            if embedding_weight_file is None:\n",
    "                raise ValueError(\n",
    "                    \"embedding_weight_file is required input with \"\n",
    "                    \"not use_character_inputs\"\n",
    "                )\n",
    "\n",
    "        self._options = options\n",
    "        self._weight_file = weight_file\n",
    "        self._embedding_weight_file = embedding_weight_file\n",
    "        self._use_character_inputs = use_character_inputs\n",
    "        self._max_batch_size = max_batch_size\n",
    "\n",
    "        self._ops = {}\n",
    "        self._graphs = {}\n",
    "\n",
    "    def __call__(self, ids_placeholder):\n",
    "        '''\n",
    "        Given the input character ids (or token ids), returns a dictionary\n",
    "            with tensorflow ops:\n",
    "\n",
    "            {'lm_embeddings': embedding_op,\n",
    "             'lengths': sequence_lengths_op,\n",
    "             'mask': op to compute mask}\n",
    "\n",
    "        embedding_op computes the LM embeddings and is shape\n",
    "            (None, 3, None, 1024)\n",
    "        lengths_op computes the sequence lengths and is shape (None, )\n",
    "        mask computes the sequence mask and is shape (None, None)\n",
    "\n",
    "        ids_placeholder: a tf.placeholder of type int32.\n",
    "            If use_character_inputs=True, it is shape\n",
    "                (None, None, max_characters_per_token) and holds the input\n",
    "                character ids for a batch\n",
    "            If use_character_input=False, it is shape (None, None) and\n",
    "                holds the input token ids for a batch\n",
    "        '''\n",
    "        if ids_placeholder in self._ops:\n",
    "            # have already created ops for this placeholder, just return them\n",
    "            ret = self._ops[ids_placeholder]\n",
    "\n",
    "        else:\n",
    "            # need to create the graph\n",
    "            if len(self._ops) == 0:\n",
    "                # first time creating the graph, don't reuse variables\n",
    "                lm_graph = BidirectionalLanguageModelGraph(\n",
    "                    self._options,\n",
    "                    self._weight_file,\n",
    "                    ids_placeholder,\n",
    "                    embedding_weight_file=self._embedding_weight_file,\n",
    "                    use_character_inputs=self._use_character_inputs,\n",
    "                    max_batch_size=self._max_batch_size)\n",
    "            else:\n",
    "                with tf.variable_scope('', reuse=True):\n",
    "                    lm_graph = BidirectionalLanguageModelGraph(\n",
    "                        self._options,\n",
    "                        self._weight_file,\n",
    "                        ids_placeholder,\n",
    "                        embedding_weight_file=self._embedding_weight_file,\n",
    "                        use_character_inputs=self._use_character_inputs,\n",
    "                        max_batch_size=self._max_batch_size)\n",
    "\n",
    "            ops = self._build_ops(lm_graph)\n",
    "            self._ops[ids_placeholder] = ops\n",
    "            self._graphs[ids_placeholder] = lm_graph\n",
    "            ret = ops\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def _build_ops(self, lm_graph):\n",
    "        with tf.control_dependencies([lm_graph.update_state_op]):\n",
    "            # get the LM embeddings\n",
    "            token_embeddings = lm_graph.embedding\n",
    "            layers = [\n",
    "                tf.concat([token_embeddings, token_embeddings], axis=2)\n",
    "            ]\n",
    "\n",
    "            n_lm_layers = len(lm_graph.lstm_outputs['forward'])\n",
    "            for i in range(n_lm_layers):\n",
    "                layers.append(\n",
    "                    tf.concat(\n",
    "                        [lm_graph.lstm_outputs['forward'][i],\n",
    "                         lm_graph.lstm_outputs['backward'][i]],\n",
    "                        axis=-1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # The layers include the BOS/EOS tokens.  Remove them\n",
    "            sequence_length_wo_bos_eos = lm_graph.sequence_lengths - 2\n",
    "            layers_without_bos_eos = []\n",
    "            for layer in layers:\n",
    "                layer_wo_bos_eos = layer[:, 1:, :]\n",
    "                layer_wo_bos_eos = tf.reverse_sequence(\n",
    "                    layer_wo_bos_eos, \n",
    "                    lm_graph.sequence_lengths - 1,\n",
    "                    seq_axis=1,\n",
    "                    batch_axis=0,\n",
    "                )\n",
    "                layer_wo_bos_eos = layer_wo_bos_eos[:, 1:, :]\n",
    "                layer_wo_bos_eos = tf.reverse_sequence(\n",
    "                    layer_wo_bos_eos,\n",
    "                    sequence_length_wo_bos_eos,\n",
    "                    seq_axis=1,\n",
    "                    batch_axis=0,\n",
    "                )\n",
    "                layers_without_bos_eos.append(layer_wo_bos_eos)\n",
    "\n",
    "            # concatenate the layers\n",
    "            lm_embeddings = tf.concat(\n",
    "                [tf.expand_dims(t, axis=1) for t in layers_without_bos_eos],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # get the mask op without bos/eos.\n",
    "            # tf doesn't support reversing boolean tensors, so cast\n",
    "            # to int then back\n",
    "            mask_wo_bos_eos = tf.cast(lm_graph.mask[:, 1:], 'int32')\n",
    "            mask_wo_bos_eos = tf.reverse_sequence(\n",
    "                mask_wo_bos_eos,\n",
    "                lm_graph.sequence_lengths - 1,\n",
    "                seq_axis=1,\n",
    "                batch_axis=0,\n",
    "            )\n",
    "            mask_wo_bos_eos = mask_wo_bos_eos[:, 1:]\n",
    "            mask_wo_bos_eos = tf.reverse_sequence(\n",
    "                mask_wo_bos_eos,\n",
    "                sequence_length_wo_bos_eos,\n",
    "                seq_axis=1,\n",
    "                batch_axis=0,\n",
    "            )\n",
    "            mask_wo_bos_eos = tf.cast(mask_wo_bos_eos, 'bool')\n",
    "\n",
    "        return {\n",
    "            'lm_embeddings': lm_embeddings, \n",
    "            'lengths': sequence_length_wo_bos_eos,\n",
    "            'token_embeddings': lm_graph.embedding,\n",
    "            'mask': mask_wo_bos_eos,\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class BidirectionalLanguageModelGraph(object):\n",
    "    '''\n",
    "    Creates the computational graph and holds the ops necessary for runnint\n",
    "    a bidirectional language model\n",
    "    '''\n",
    "    def __init__(self, options, weight_file, ids_placeholder,\n",
    "                 use_character_inputs=True, embedding_weight_file=None,\n",
    "                 max_batch_size=128):\n",
    "\n",
    "        self.options = options\n",
    "        self._max_batch_size = max_batch_size\n",
    "        self.ids_placeholder = ids_placeholder\n",
    "        self.use_character_inputs = use_character_inputs\n",
    "\n",
    "        # this custom_getter will make all variables not trainable and\n",
    "        # override the default initializer\n",
    "        def custom_getter(getter, name, *args, **kwargs):\n",
    "            kwargs['trainable'] = False\n",
    "            kwargs['initializer'] = _pretrained_initializer(\n",
    "                name, weight_file, embedding_weight_file\n",
    "            )\n",
    "            return getter(name, *args, **kwargs)\n",
    "\n",
    "        if embedding_weight_file is not None:\n",
    "            # get the vocab size\n",
    "            with h5py.File(embedding_weight_file, 'r') as fin:\n",
    "                # +1 for padding\n",
    "                self._n_tokens_vocab = fin['embedding'].shape[0] + 1\n",
    "        else:\n",
    "            self._n_tokens_vocab = None\n",
    "\n",
    "        with tf.variable_scope('bilm', custom_getter=custom_getter):\n",
    "            self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        if self.use_character_inputs:\n",
    "            self._build_word_char_embeddings()\n",
    "        else:\n",
    "            self._build_word_embeddings()\n",
    "        self._build_lstms()\n",
    "\n",
    "    def _build_word_char_embeddings(self):\n",
    "        '''\n",
    "        options contains key 'char_cnn': {\n",
    "\n",
    "        'n_characters': 262,\n",
    "\n",
    "        # includes the start / end characters\n",
    "        'max_characters_per_token': 50,\n",
    "\n",
    "        'filters': [\n",
    "            [1, 32],\n",
    "            [2, 32],\n",
    "            [3, 64],\n",
    "            [4, 128],\n",
    "            [5, 256],\n",
    "            [6, 512],\n",
    "            [7, 512]\n",
    "        ],\n",
    "        'activation': 'tanh',\n",
    "\n",
    "        # for the character embedding\n",
    "        'embedding': {'dim': 16}\n",
    "\n",
    "        # for highway layers\n",
    "        # if omitted, then no highway layers\n",
    "        'n_highway': 2,\n",
    "        }\n",
    "        '''\n",
    "        projection_dim = self.options['lstm']['projection_dim']\n",
    "\n",
    "        cnn_options = self.options['char_cnn']\n",
    "        filters = cnn_options['filters']\n",
    "        n_filters = sum(f[1] for f in filters)\n",
    "        max_chars = cnn_options['max_characters_per_token']\n",
    "        char_embed_dim = cnn_options['embedding']['dim']\n",
    "        n_chars = cnn_options['n_characters'] + 1\n",
    "        if n_chars != 262:\n",
    "            raise InvalidNumberOfCharacters(\n",
    "                \"Set n_characters=262 after training see the README.md\"\n",
    "            )\n",
    "        if cnn_options['activation'] == 'tanh':\n",
    "            activation = tf.nn.tanh\n",
    "        elif cnn_options['activation'] == 'relu':\n",
    "            activation = tf.nn.relu\n",
    "\n",
    "        # the character embeddings\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.embedding_weights = tf.get_variable(\n",
    "                    \"char_embed\", [n_chars, char_embed_dim],\n",
    "                    dtype=DTYPE,\n",
    "                    initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "            )\n",
    "            # shape (batch_size, unroll_steps, max_chars, embed_dim)\n",
    "            self.char_embedding = tf.nn.embedding_lookup(self.embedding_weights,\n",
    "                                                    self.ids_placeholder)\n",
    "\n",
    "        # the convolutions\n",
    "        def make_convolutions(inp):\n",
    "            with tf.variable_scope('CNN') as scope:\n",
    "                convolutions = []\n",
    "                for i, (width, num) in enumerate(filters):\n",
    "                    if cnn_options['activation'] == 'relu':\n",
    "                        # He initialization for ReLU activation\n",
    "                        # with char embeddings init between -1 and 1\n",
    "                        #w_init = tf.random_normal_initializer(\n",
    "                        #    mean=0.0,\n",
    "                        #    stddev=np.sqrt(2.0 / (width * char_embed_dim))\n",
    "                        #)\n",
    "\n",
    "                        # Kim et al 2015, +/- 0.05\n",
    "                        w_init = tf.random_uniform_initializer(\n",
    "                            minval=-0.05, maxval=0.05)\n",
    "                    elif cnn_options['activation'] == 'tanh':\n",
    "                        # glorot init\n",
    "                        w_init = tf.random_normal_initializer(\n",
    "                            mean=0.0,\n",
    "                            stddev=np.sqrt(1.0 / (width * char_embed_dim))\n",
    "                        )\n",
    "                    w = tf.get_variable(\n",
    "                        \"W_cnn_%s\" % i,\n",
    "                        [1, width, char_embed_dim, num],\n",
    "                        initializer=w_init,\n",
    "                        dtype=DTYPE)\n",
    "                    b = tf.get_variable(\n",
    "                        \"b_cnn_%s\" % i, [num], dtype=DTYPE,\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "                    conv = tf.nn.conv2d(\n",
    "                            inp, w,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding=\"VALID\") + b\n",
    "                    # now max pool\n",
    "                    conv = tf.nn.max_pool(\n",
    "                            conv, [1, 1, max_chars-width+1, 1],\n",
    "                            [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "                    # activation\n",
    "                    conv = activation(conv)\n",
    "                    conv = tf.squeeze(conv, squeeze_dims=[2])\n",
    "\n",
    "                    convolutions.append(conv)\n",
    "\n",
    "            return tf.concat(convolutions, 2)\n",
    "\n",
    "        embedding = make_convolutions(self.char_embedding)\n",
    "\n",
    "        # for highway and projection layers\n",
    "        n_highway = cnn_options.get('n_highway')\n",
    "        use_highway = n_highway is not None and n_highway > 0\n",
    "        use_proj = n_filters != projection_dim\n",
    "\n",
    "        if use_highway or use_proj:\n",
    "            #   reshape from (batch_size, n_tokens, dim) to (-1, dim)\n",
    "            batch_size_n_tokens = tf.shape(embedding)[0:2]\n",
    "            embedding = tf.reshape(embedding, [-1, n_filters])\n",
    "\n",
    "        # set up weights for projection\n",
    "        if use_proj:\n",
    "            assert n_filters > projection_dim\n",
    "            with tf.variable_scope('CNN_proj') as scope:\n",
    "                    W_proj_cnn = tf.get_variable(\n",
    "                        \"W_proj\", [n_filters, projection_dim],\n",
    "                        initializer=tf.random_normal_initializer(\n",
    "                            mean=0.0, stddev=np.sqrt(1.0 / n_filters)),\n",
    "                        dtype=DTYPE)\n",
    "                    b_proj_cnn = tf.get_variable(\n",
    "                        \"b_proj\", [projection_dim],\n",
    "                        initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=DTYPE)\n",
    "\n",
    "        # apply highways layers\n",
    "        def high(x, ww_carry, bb_carry, ww_tr, bb_tr):\n",
    "            carry_gate = tf.nn.sigmoid(tf.matmul(x, ww_carry) + bb_carry)\n",
    "            transform_gate = tf.nn.relu(tf.matmul(x, ww_tr) + bb_tr)\n",
    "            return carry_gate * transform_gate + (1.0 - carry_gate) * x\n",
    "\n",
    "        if use_highway:\n",
    "            highway_dim = n_filters\n",
    "\n",
    "            for i in range(n_highway):\n",
    "                with tf.variable_scope('CNN_high_%s' % i) as scope:\n",
    "                    W_carry = tf.get_variable(\n",
    "                        'W_carry', [highway_dim, highway_dim],\n",
    "                        # glorit init\n",
    "                        initializer=tf.random_normal_initializer(\n",
    "                            mean=0.0, stddev=np.sqrt(1.0 / highway_dim)),\n",
    "                        dtype=DTYPE)\n",
    "                    b_carry = tf.get_variable(\n",
    "                        'b_carry', [highway_dim],\n",
    "                        initializer=tf.constant_initializer(-2.0),\n",
    "                        dtype=DTYPE)\n",
    "                    W_transform = tf.get_variable(\n",
    "                        'W_transform', [highway_dim, highway_dim],\n",
    "                        initializer=tf.random_normal_initializer(\n",
    "                            mean=0.0, stddev=np.sqrt(1.0 / highway_dim)),\n",
    "                        dtype=DTYPE)\n",
    "                    b_transform = tf.get_variable(\n",
    "                        'b_transform', [highway_dim],\n",
    "                        initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=DTYPE)\n",
    "\n",
    "                embedding = high(embedding, W_carry, b_carry,\n",
    "                                 W_transform, b_transform)\n",
    "\n",
    "        # finally project down if needed\n",
    "        if use_proj:\n",
    "            embedding = tf.matmul(embedding, W_proj_cnn) + b_proj_cnn\n",
    "\n",
    "        # reshape back to (batch_size, tokens, dim)\n",
    "        if use_highway or use_proj:\n",
    "            shp = tf.concat([batch_size_n_tokens, [projection_dim]], axis=0)\n",
    "            embedding = tf.reshape(embedding, shp)\n",
    "\n",
    "        # at last assign attributes for remainder of the model\n",
    "        self.embedding = embedding\n",
    "\n",
    "\n",
    "    def _build_word_embeddings(self):\n",
    "        projection_dim = self.options['lstm']['projection_dim']\n",
    "\n",
    "        # the word embeddings\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.embedding_weights = tf.get_variable(\n",
    "                \"embedding\", [self._n_tokens_vocab, projection_dim],\n",
    "                dtype=DTYPE,\n",
    "            )\n",
    "            self.embedding = tf.nn.embedding_lookup(self.embedding_weights,\n",
    "                                                self.ids_placeholder)\n",
    "\n",
    "\n",
    "    def _build_lstms(self):\n",
    "        # now the LSTMs\n",
    "        # these will collect the initial states for the forward\n",
    "        #   (and reverse LSTMs if we are doing bidirectional)\n",
    "\n",
    "        # parse the options\n",
    "        lstm_dim = self.options['lstm']['dim']\n",
    "        projection_dim = self.options['lstm']['projection_dim']\n",
    "        n_lstm_layers = self.options['lstm'].get('n_layers', 1)\n",
    "        cell_clip = self.options['lstm'].get('cell_clip')\n",
    "        proj_clip = self.options['lstm'].get('proj_clip')\n",
    "        use_skip_connections = self.options['lstm']['use_skip_connections']\n",
    "        if use_skip_connections:\n",
    "            print(\"USING SKIP CONNECTIONS\")\n",
    "        else:\n",
    "            print(\"NOT USING SKIP CONNECTIONS\")\n",
    "\n",
    "        # the sequence lengths from input mask\n",
    "        if self.use_character_inputs:\n",
    "            mask = tf.reduce_any(self.ids_placeholder > 0, axis=2)\n",
    "        else:\n",
    "            mask = self.ids_placeholder > 0\n",
    "        sequence_lengths = tf.reduce_sum(tf.cast(mask, tf.int32), axis=1)\n",
    "        batch_size = tf.shape(sequence_lengths)[0]\n",
    "\n",
    "        # for each direction, we'll store tensors for each layer\n",
    "        self.lstm_outputs = {'forward': [], 'backward': []}\n",
    "        self.lstm_state_sizes = {'forward': [], 'backward': []}\n",
    "        self.lstm_init_states = {'forward': [], 'backward': []}\n",
    "        self.lstm_final_states = {'forward': [], 'backward': []}\n",
    "\n",
    "        update_ops = []\n",
    "        for direction in ['forward', 'backward']:\n",
    "            if direction == 'forward':\n",
    "                layer_input = self.embedding\n",
    "            else:\n",
    "                layer_input = tf.reverse_sequence(\n",
    "                    self.embedding,\n",
    "                    sequence_lengths,\n",
    "                    seq_axis=1,\n",
    "                    batch_axis=0\n",
    "                )\n",
    "\n",
    "            for i in range(n_lstm_layers):\n",
    "                if projection_dim < lstm_dim:\n",
    "                    # are projecting down output\n",
    "                    lstm_cell = tf.nn.rnn_cell.LSTMCell(\n",
    "                        lstm_dim, num_proj=projection_dim,\n",
    "                        cell_clip=cell_clip, proj_clip=proj_clip)\n",
    "                else:\n",
    "                    lstm_cell = tf.nn.rnn_cell.LSTMCell(\n",
    "                            lstm_dim,\n",
    "                            cell_clip=cell_clip, proj_clip=proj_clip)\n",
    "\n",
    "                if use_skip_connections:\n",
    "                    # ResidualWrapper adds inputs to outputs\n",
    "                    if i == 0:\n",
    "                        # don't add skip connection from token embedding to\n",
    "                        # 1st layer output\n",
    "                        pass\n",
    "                    else:\n",
    "                        # add a skip connection\n",
    "                        lstm_cell = tf.nn.rnn_cell.ResidualWrapper(lstm_cell)\n",
    "\n",
    "                # collect the input state, run the dynamic rnn, collect\n",
    "                # the output\n",
    "                state_size = lstm_cell.state_size\n",
    "                # the LSTMs are stateful.  To support multiple batch sizes,\n",
    "                # we'll allocate size for states up to max_batch_size,\n",
    "                # then use the first batch_size entries for each batch\n",
    "                init_states = [\n",
    "                    tf.Variable(\n",
    "                        tf.zeros([self._max_batch_size, dim]),\n",
    "                        trainable=False\n",
    "                    )\n",
    "                    for dim in lstm_cell.state_size\n",
    "                ]\n",
    "                batch_init_states = [\n",
    "                    state[:batch_size, :] for state in init_states\n",
    "                ]\n",
    "\n",
    "                if direction == 'forward':\n",
    "                    i_direction = 0\n",
    "                else:\n",
    "                    i_direction = 1\n",
    "                variable_scope_name = 'RNN_{0}/RNN/MultiRNNCell/Cell{1}'.format(\n",
    "                    i_direction, i)\n",
    "                with tf.variable_scope(variable_scope_name):\n",
    "                    layer_output, final_state = tf.nn.dynamic_rnn(\n",
    "                        lstm_cell,\n",
    "                        layer_input,\n",
    "                        sequence_length=sequence_lengths,\n",
    "                        initial_state=tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                            *batch_init_states),\n",
    "                    )\n",
    "\n",
    "                self.lstm_state_sizes[direction].append(lstm_cell.state_size)\n",
    "                self.lstm_init_states[direction].append(init_states)\n",
    "                self.lstm_final_states[direction].append(final_state)\n",
    "                if direction == 'forward':\n",
    "                    self.lstm_outputs[direction].append(layer_output)\n",
    "                else:\n",
    "                    self.lstm_outputs[direction].append(\n",
    "                        tf.reverse_sequence(\n",
    "                            layer_output,\n",
    "                            sequence_lengths,\n",
    "                            seq_axis=1,\n",
    "                            batch_axis=0\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                with tf.control_dependencies([layer_output]):\n",
    "                    # update the initial states\n",
    "                    for i in range(2):\n",
    "                        new_state = tf.concat(\n",
    "                            [final_state[i][:batch_size, :],\n",
    "                             init_states[i][batch_size:, :]], axis=0)\n",
    "                        state_update_op = tf.assign(init_states[i], new_state)\n",
    "                        update_ops.append(state_update_op)\n",
    "    \n",
    "                layer_input = layer_output\n",
    "\n",
    "        self.mask = mask\n",
    "        self.sequence_lengths = sequence_lengths\n",
    "        self.update_state_op = tf.group(*update_ops)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def weight_layers(name, bilm_ops, l2_coef=None,\n",
    "                  use_top_only=False, do_layer_norm=False):\n",
    "    '''\n",
    "    Weight the layers of a biLM with trainable scalar weights to\n",
    "    compute ELMo representations.\n",
    "\n",
    "    For each output layer, this returns two ops.  The first computes\n",
    "        a layer specific weighted average of the biLM layers, and\n",
    "        the second the l2 regularizer loss term.\n",
    "    The regularization terms are also add to tf.GraphKeys.REGULARIZATION_LOSSES \n",
    "\n",
    "    Input:\n",
    "        name = a string prefix used for the trainable variable names\n",
    "        bilm_ops = the tensorflow ops returned to compute internal\n",
    "            representations from a biLM.  This is the return value\n",
    "            from BidirectionalLanguageModel(...)(ids_placeholder)\n",
    "        l2_coef: the l2 regularization coefficient $\\lambda$.\n",
    "            Pass None or 0.0 for no regularization.\n",
    "        use_top_only: if True, then only use the top layer.\n",
    "        do_layer_norm: if True, then apply layer normalization to each biLM\n",
    "            layer before normalizing\n",
    "\n",
    "    Output:\n",
    "        {\n",
    "            'weighted_op': op to compute weighted average for output,\n",
    "            'regularization_op': op to compute regularization term\n",
    "        }\n",
    "    '''\n",
    "    def _l2_regularizer(weights):\n",
    "        if l2_coef is not None:\n",
    "            return l2_coef * tf.reduce_sum(tf.square(weights))\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    # Get ops for computing LM embeddings and mask\n",
    "    lm_embeddings = bilm_ops['lm_embeddings']\n",
    "    mask = bilm_ops['mask']\n",
    "\n",
    "    n_lm_layers = int(lm_embeddings.get_shape()[1])\n",
    "    lm_dim = int(lm_embeddings.get_shape()[3])\n",
    "\n",
    "    with tf.control_dependencies([lm_embeddings, mask]):\n",
    "        # Cast the mask and broadcast for layer use.\n",
    "        mask_float = tf.cast(mask, 'float32')\n",
    "        broadcast_mask = tf.expand_dims(mask_float, axis=-1)\n",
    "\n",
    "        def _do_ln(x):\n",
    "            # do layer normalization excluding the mask\n",
    "            x_masked = x * broadcast_mask\n",
    "            N = tf.reduce_sum(mask_float) * lm_dim\n",
    "            mean = tf.reduce_sum(x_masked) / N\n",
    "            variance = tf.reduce_sum(((x_masked - mean) * broadcast_mask)**2) / N\n",
    "            return tf.nn.batch_normalization(\n",
    "                x, mean, variance, None, None, 1E-12\n",
    "            )\n",
    "\n",
    "        if use_top_only:\n",
    "            layers = tf.split(lm_embeddings, n_lm_layers, axis=1)\n",
    "            # just the top layer\n",
    "            sum_pieces = tf.squeeze(layers[-1], squeeze_dims=1)\n",
    "            # no regularization\n",
    "            reg = 0.0\n",
    "        else:\n",
    "            W = tf.get_variable(\n",
    "                '{}_ELMo_W'.format(name),\n",
    "                shape=(n_lm_layers, ),\n",
    "                initializer=tf.zeros_initializer,\n",
    "                regularizer=_l2_regularizer,\n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "            # normalize the weights\n",
    "            normed_weights = tf.split(\n",
    "                tf.nn.softmax(W + 1.0 / n_lm_layers), n_lm_layers\n",
    "            )\n",
    "            # split LM layers\n",
    "            layers = tf.split(lm_embeddings, n_lm_layers, axis=1)\n",
    "    \n",
    "            # compute the weighted, normalized LM activations\n",
    "            pieces = []\n",
    "            for w, t in zip(normed_weights, layers):\n",
    "                if do_layer_norm:\n",
    "                    pieces.append(w * _do_ln(tf.squeeze(t, squeeze_dims=1)))\n",
    "                else:\n",
    "                    pieces.append(w * tf.squeeze(t, squeeze_dims=1))\n",
    "            sum_pieces = tf.add_n(pieces)\n",
    "    \n",
    "            # get the regularizer \n",
    "            reg = [\n",
    "                r for r in tf.get_collection(\n",
    "                                tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "                if r.name.find('{}_ELMo_W/'.format(name)) >= 0\n",
    "            ]\n",
    "            if len(reg) != 1:\n",
    "                raise ValueError\n",
    "\n",
    "        # scale the weighted sum by gamma\n",
    "        gamma = tf.get_variable(\n",
    "            '{}_ELMo_gamma'.format(name),\n",
    "            shape=(1, ),\n",
    "            initializer=tf.ones_initializer,\n",
    "            regularizer=None,\n",
    "            trainable=True,\n",
    "        )\n",
    "        weighted_lm_layers = sum_pieces * gamma\n",
    "\n",
    "        ret = {'weighted_op': weighted_lm_layers, 'regularization_op': reg}\n",
    "\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo 파인 튜닝 네트워크의 텐서 그래프 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_elmo_graph(options_fname, pretrain_model_fname, max_characters_per_token, num_labels, tune=False):\n",
    "    \"\"\"\n",
    "        ids_placeholder : ELMo 네트워크의 입력값 (ids)\n",
    "            - shape : [batch_size, unroll_steps, max_character_byte_length]\n",
    "        elmo_embeddings : fine tuning 네트워크의 입력값 (ELMo 네트워크의 출력값)\n",
    "            - shape : [batch_size, unroll_steps, dimension]\n",
    "        labels_placeholder : fine tuning 네트워크의 출력값 (예 : 긍정=1/부정=0)\n",
    "            - shape : [batch_size]\n",
    "        loss : fine tuning 네트워크의 loss\n",
    "    \"\"\"\n",
    "    # Build the biLM graph.\n",
    "    # Load pretrained ELMo model.\n",
    "    # 문자 수준 CNN, 양방향 LSTM 레이어에 해당하는 텐서플로 계산 그래프 생성, 프리트레인된 학습 파라미터 읽음\n",
    "    bilm = BidirectionalLanguageModel(options_fname, pretrain_model_fname)\n",
    "    # Input placeholders to the biLM.\n",
    "    # 각 단어 ID 시퀀스를 받는 입력 텐서\n",
    "    ids_placeholder = tf.placeholder(tf.int32, shape=(None, None, max_characters_per_token), name='input')\n",
    "    if tune:\n",
    "        # Output placeholders to the fine-tuned Net.\n",
    "        labels_placeholder = tf.placeholder(tf.int32, shape=(None))\n",
    "    else:\n",
    "        labels_placeholder = None\n",
    "    # Get ops to compute the LM embeddings.\n",
    "    # 이 객체는 ELMo 임베딩을 계산할 밑바탕이 되는 임베딩(문자 수준 CNN 출력 벡터, 양방향 LSTM 레이어 출력 벡터)을 리턴\n",
    "    embeddings_op = bilm(ids_placeholder)\n",
    "    # Get lengths.\n",
    "    input_lengths = embeddings_op['lengths']\n",
    "    # define dropout\n",
    "    if tune:\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "    else:\n",
    "        dropout_keep_prob = tf.constant(1.0, dtype=tf.float32)\n",
    "    # the ELMo layer\n",
    "    # shape : [batch_size, unroll_steps, dimension]\n",
    "    # 여기에 각 가중치를 곱해 가중합 수행\n",
    "    # ELMo 임베딩 획득\n",
    "    # 적용되는 가중치들은 파인 튜닝을 하면서 다른 학습 파라미터들과 함께 업데이트\n",
    "    elmo_embeddings = weight_layers(\"elmo_embeddings\",\n",
    "                                    embeddings_op,\n",
    "                                    l2_coef=0.0,\n",
    "                                    use_top_only=False,\n",
    "                                    do_layer_norm=True)\n",
    "    # input of fine tuning network-> 비로소 파인튜닝의 입력 값인 최종 엘모 임베딩 결과\n",
    "    features = tf.nn.dropout(elmo_embeddings['weighted_op'], dropout_keep_prob)\n",
    "    # Bidirectional LSTM Layer\n",
    "    # forward, backward\n",
    "    lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(num_units=512,\n",
    "                                           cell_clip=5,\n",
    "                                           proj_clip=5)\n",
    "    lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(num_units=512,\n",
    "                                           cell_clip=5,\n",
    "                                           proj_clip=5)\n",
    "    lstm_output, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_cell_fw,\n",
    "                                                     cell_bw=lstm_cell_bw,\n",
    "                                                     inputs=features,\n",
    "                                                     sequence_length=input_lengths,\n",
    "                                                     dtype=tf.float32)\n",
    "\n",
    "    # Attention Layer\n",
    "    output_fw, output_bw = lstm_output\n",
    "    H = tf.contrib.layers.fully_connected(inputs=output_fw + output_bw, num_outputs=256, activation_fn=tf.nn.tanh)\n",
    "    attention_score = tf.nn.softmax(tf.contrib.layers.fully_connected(inputs=H, num_outputs=1, activation_fn=None), axis=1)\n",
    "    attention_output = tf.squeeze(tf.matmul(tf.transpose(H, perm=[0, 2, 1]), attention_score), axis=-1)\n",
    "    layer_output = tf.nn.dropout(attention_output, dropout_keep_prob)\n",
    "\n",
    "    # Feed-Forward Layer\n",
    "    fc = tf.contrib.layers.fully_connected(inputs=layer_output,\n",
    "                                           num_outputs=512,\n",
    "                                           activation_fn=tf.nn.relu,\n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           biases_initializer=tf.zeros_initializer())\n",
    "    features_drop = tf.nn.dropout(fc, dropout_keep_prob)\n",
    "    logits = tf.contrib.layers.fully_connected(inputs=features_drop,\n",
    "                                               num_outputs=num_labels,\n",
    "                                               activation_fn=None,\n",
    "                                               weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                               biases_initializer=tf.zeros_initializer())\n",
    "    if tune:\n",
    "        # Loss Layer\n",
    "        CE = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_placeholder, logits=logits)\n",
    "        loss = tf.reduce_mean(CE)\n",
    "        return ids_placeholder, labels_placeholder, dropout_keep_prob, logits, loss\n",
    "    else:\n",
    "        # prob Layer\n",
    "        probs = tf.nn.softmax(logits, axis=-1, name='probs')\n",
    "        return ids_placeholder, elmo_embeddings, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMoTuner 클래스 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuner 를 상속받는 자식 클래스 -> ELMo 파인 튜닝 네트워크를 학습하는 역할\n",
    "# 부모 클래스 튜너외에 추가로 하는 작업은 \n",
    "# (1) 입력 단어들을 ID로 변환하는 역할을 하는 Batcher를 정의\n",
    "# (2) ELMo 파인 튜닝 네트워크의 텐서 그래프를 그리는 일\n",
    "\n",
    "class ELMoTuner(Tuner):\n",
    "\n",
    "    # 선언부\n",
    "    def __init__(self, train_corpus_fname, test_corpus_fname,\n",
    "                 vocab_fname, options_fname, pretrain_model_fname,\n",
    "                 model_save_path, max_characters_per_token=30,\n",
    "                 batch_size=32, num_labels=2):\n",
    "        # Load a corpus.\n",
    "        super().__init__(train_corpus_fname=train_corpus_fname,\n",
    "                         tokenized_train_corpus_fname=train_corpus_fname + \".elmo-tokenized\",\n",
    "                         test_corpus_fname=test_corpus_fname,\n",
    "                         tokenized_test_corpus_fname=test_corpus_fname + \".elmo-tokenized\",\n",
    "                         model_name=\"elmo\", vocab_fname=vocab_fname,\n",
    "                         model_save_path=model_save_path, batch_size=batch_size)\n",
    "        # configurations\n",
    "        self.options_fname = options_fname\n",
    "        self.pretrain_model_fname = pretrain_model_fname\n",
    "        self.max_characters_per_token = max_characters_per_token\n",
    "        self.num_labels = 2 # positive, negative\n",
    "        self.num_train_steps = (int((len(self.train_data) - 1) / self.batch_size) + 1) * self.num_epochs\n",
    "        self.eval_every = int(self.num_train_steps / self.num_epochs)  # epoch마다 평가\n",
    "        # Create a Batcher to map text to character ids.-> 입력 단어들을 ID로 변환하는 역할을 하는 Batcher\n",
    "        # lm_vocab_file = ELMo는 token vocab이 없어도 on-the-fly로 입력 id들을 만들 수 있다\n",
    "        \n",
    "        # ELMo 모델은 문자 단위의 입력(유니코드)을 받고, \n",
    "        # 파인 튜닝 과정에서는 입력 단어 시퀀스 다음 단어가 무엇일지 예측하는 과정이 생략되기 때문에 어휘 집합 필요무\n",
    "        # 하지만 자주 나오는 char sequence, 즉 vocab을 미리 id로 만들어 놓으면 좀 더 빠른 학습이 가능\n",
    "        # max_token_length = the maximum number of characters in each token\n",
    "        self.batcher = Batcher(lm_vocab_file=vocab_fname, max_token_length=self.max_characters_per_token)\n",
    "        self.training = tf.placeholder(tf.bool)\n",
    "        # build train graph\n",
    "        self.ids_placeholder, self.labels_placeholder, self.dropout_keep_prob, self.logits, self.loss = make_elmo_graph(options_fname,\n",
    "                                                                                                                        pretrain_model_fname,\n",
    "                                                                                                                        max_characters_per_token,\n",
    "                                                                                                                        num_labels, tune=True)\n",
    "    # 옵티마이저 등 정의\n",
    "    # 이 함수 호출시 ELMo 파인 튜닝 네트워크의 학습이 시작됨\n",
    "    def tune(self):\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        output_feed = [train_op, global_step, self.logits, self.loss]\n",
    "        saver = tf.train.Saver(max_to_keep=1)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        self.train(sess, saver, global_step, output_feed)\n",
    "\n",
    "    # 입력값 생성 함수\n",
    "    def make_input(self, sentences, labels, is_training):\n",
    "        # 배치 문장들을 유니코드 ID들로 변환\n",
    "        current_input = self.batcher.batch_sentences(sentences)\n",
    "        current_output = np.array(labels)\n",
    "        if is_training:\n",
    "            input_feed = {\n",
    "                self.ids_placeholder: current_input,\n",
    "                self.labels_placeholder: current_output,\n",
    "                self.dropout_keep_prob: self.dropout_keep_prob_rate,\n",
    "                self.training: True\n",
    "            }\n",
    "        else:\n",
    "            input_feed_ = {\n",
    "                self.ids_placeholder: current_input,\n",
    "                self.labels_placeholder: current_output,\n",
    "                self.dropout_keep_prob: 1.0,\n",
    "                self.training: False\n",
    "            }\n",
    "            input_feed = [input_feed_, current_output]\n",
    "        return input_feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbeddingEvaluator:\n",
    "\n",
    "    def __init__(self, model_name, dimension, use_notebook=False):\n",
    "        # reset graphs.\n",
    "        tf.reset_default_graph()\n",
    "        self.model_name = model_name\n",
    "        self.dimension = dimension\n",
    "        self.use_notebook = use_notebook\n",
    "\n",
    "    def get_token_vector_sequence(self, sentence):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_sentence_vector(self, sentence):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def make_input(self, tokens):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visualize_homonym(self, homonym, sentences, palette=\"Viridis256\"):\n",
    "        tokenized_sentences = []\n",
    "        vecs = np.zeros((1, self.dimension))\n",
    "        for sentence in sentences:\n",
    "            tokens, vec = self.get_token_vector_sequence(sentence)\n",
    "            tokenized_sentences.append(tokens)\n",
    "            vecs = np.concatenate([vecs, vec], axis=0)\n",
    "        visualize_homonym(homonym, tokenized_sentences, vecs, self.model_name, palette, use_notebook=self.use_notebook)\n",
    "\n",
    "    def visualize_sentences(self, sentences, palette=\"Viridis256\"):\n",
    "        vecs = np.array([self.get_sentence_vector(sentence)[1] for sentence in sentences])\n",
    "        visualize_sentences(vecs, sentences, palette, use_notebook=self.use_notebook)\n",
    "\n",
    "    def visualize_between_sentences(self, sentences, palette=\"Viridis256\"):\n",
    "        vec_list = []\n",
    "        for sentence in sentences:\n",
    "            _, vec = self.get_sentence_vector(sentence)\n",
    "            vec_list.append(vec)\n",
    "        visualize_between_sentences(sentences, vec_list, palette, use_notebook=self.use_notebook)\n",
    "\n",
    "        \n",
    "        \n",
    "# ELMo를 평가하기 위한 평가 클래스\n",
    "class ELMoEmbeddingEvaluator(SentenceEmbeddingEvaluator):\n",
    "\n",
    "    def __init__(self, tune_model_fname=\"/notebooks/embedding/data/sentence-embeddings/elmo/tune-ckpt\",\n",
    "                 pretrain_model_fname=\"/notebooks/embedding/data/sentence-embeddings/elmo/pretrain-ckpt/elmo.model\",\n",
    "                 options_fname=\"/notebooks/embedding/data/sentence-embeddings/elmo/pretrain-ckpt/options.json\",\n",
    "                 vocab_fname=\"/notebooks/embedding/data/sentence-embeddings/elmo/pretrain-ckpt/elmo-vocab.txt\",\n",
    "                 max_characters_per_token=30, dimension=256, num_labels=2, use_notebook=False):\n",
    "\n",
    "        # configurations\n",
    "        super().__init__(\"elmo\", dimension, use_notebook)\n",
    "        self.tokenizer = get_tokenizer(\"mecab\")\n",
    "        self.batcher = Batcher(lm_vocab_file=vocab_fname, max_token_length=max_characters_per_token)\n",
    "        self.ids_placeholder, self.elmo_embeddings, self.probs = make_elmo_graph(options_fname,\n",
    "                                                                                 pretrain_model_fname,\n",
    "                                                                                 max_characters_per_token,\n",
    "                                                                                 num_labels, tune=False)\n",
    "        # restore model\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        self.sess = tf.Session()\n",
    "        checkpoint_path = tf.train.latest_checkpoint(tune_model_fname)\n",
    "        saver.restore(self.sess, checkpoint_path)\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        tokens = self.tokenize(sentence)\n",
    "        model_input = self.make_input(tokens)\n",
    "        probs = self.sess.run(self.probs, model_input)\n",
    "        return probs\n",
    "\n",
    "    \"\"\"\n",
    "    sentence를 입력하면 토크나이즈 결과와 token 벡터 시퀀스를 반환한다\n",
    "        - shape :[[# of tokens], [batch size, max seq length, dimension]]\n",
    "    \"\"\"\n",
    "    def get_token_vector_sequence(self, sentence):\n",
    "        tokens = self.tokenize(sentence)\n",
    "        model_input = self.make_input(tokens)\n",
    "        sentence_vector = self.sess.run(self.elmo_embeddings['weighted_op'], model_input)\n",
    "        return [tokens, sentence_vector[0]]\n",
    "\n",
    "    \"\"\"\n",
    "    sentence를 입력하면 토크나이즈 결과와 토큰 시퀀스의 마지막 벡터를 반환한다\n",
    "    ELMo는 Language Model이기 때문에 토큰 시퀀스 마지막 벡터에 많은 정보가 녹아 있다\n",
    "         - shape :[[# of tokens], [batch size, dimension]]\n",
    "    \"\"\"\n",
    "    def get_sentence_vector(self, sentence):\n",
    "        tokens, vecs = self.get_token_vector_sequence(sentence)\n",
    "        return [tokens, vecs[-1]]\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        tokens = self.tokenizer.morphs(sentence)\n",
    "        return post_processing(tokens)\n",
    "\n",
    "    def make_input(self, tokens):\n",
    "        model_input = self.batcher.batch_sentences([tokens])\n",
    "        input_feed = {self.ids_placeholder: model_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ELMoEmbeddingEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "#from keras import backend as K\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "nata = pd.read_table(\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_elmo_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
